{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TAKE A QUICK LOOK ON TRAIN DATA\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is classified as a 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's up man?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 0][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train[\"target\"] == 1][\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AND TEST DATA\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=train, x='target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos parecen relativamente bien balanceados....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ###  NaNs TRAIN   #####\n",
      " id             0\n",
      "keyword       61\n",
      "location    2533\n",
      "text           0\n",
      "target         0\n",
      "dtype: int64\n",
      " \n",
      "###  NaNs TEST   #####\n",
      " id             0\n",
      "keyword       26\n",
      "location    1105\n",
      "text           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(' ###  NaNs TRAIN   #####\\n',train.isnull().sum())\n",
    "print(' \\n###  NaNs TEST   #####\\n',test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of locations:  [nan 'Birmingham' 'Est. September 2012 - Bristol' ... 'Vancouver, Canada'\n",
      " 'London ' 'Lincoln']\n",
      "3341  different unique locations\n"
     ]
    }
   ],
   "source": [
    "# some examples of location\n",
    "print('Some examples of locations: ',train.location.unique() )\n",
    "# how many different locations?\n",
    "print(train.location.nunique(),' different unique locations' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of keyword:  [nan 'ablaze' 'accident' 'aftershock' 'airplane%20accident' 'ambulance'\n",
      " 'annihilated' 'annihilation' 'apocalypse' 'armageddon' 'army' 'arson'\n",
      " 'arsonist' 'attack' 'attacked' 'avalanche' 'battle' 'bioterror'\n",
      " 'bioterrorism' 'blaze' 'blazing' 'bleeding' 'blew%20up' 'blight'\n",
      " 'blizzard' 'blood' 'bloody' 'blown%20up' 'body%20bag' 'body%20bagging'\n",
      " 'body%20bags' 'bomb' 'bombed' 'bombing' 'bridge%20collapse'\n",
      " 'buildings%20burning' 'buildings%20on%20fire' 'burned' 'burning'\n",
      " 'burning%20buildings' 'bush%20fires' 'casualties' 'casualty'\n",
      " 'catastrophe' 'catastrophic' 'chemical%20emergency' 'cliff%20fall'\n",
      " 'collapse' 'collapsed' 'collide' 'collided' 'collision' 'crash' 'crashed'\n",
      " 'crush' 'crushed' 'curfew' 'cyclone' 'damage' 'danger' 'dead' 'death'\n",
      " 'deaths' 'debris' 'deluge' 'deluged' 'demolish' 'demolished' 'demolition'\n",
      " 'derail' 'derailed' 'derailment' 'desolate' 'desolation' 'destroy'\n",
      " 'destroyed' 'destruction' 'detonate' 'detonation' 'devastated'\n",
      " 'devastation' 'disaster' 'displaced' 'drought' 'drown' 'drowned'\n",
      " 'drowning' 'dust%20storm' 'earthquake' 'electrocute' 'electrocuted'\n",
      " 'emergency' 'emergency%20plan' 'emergency%20services' 'engulfed'\n",
      " 'epicentre' 'evacuate' 'evacuated' 'evacuation' 'explode' 'exploded'\n",
      " 'explosion' 'eyewitness' 'famine' 'fatal' 'fatalities' 'fatality' 'fear'\n",
      " 'fire' 'fire%20truck' 'first%20responders' 'flames' 'flattened' 'flood'\n",
      " 'flooding' 'floods' 'forest%20fire' 'forest%20fires' 'hail' 'hailstorm'\n",
      " 'harm' 'hazard' 'hazardous' 'heat%20wave' 'hellfire' 'hijack' 'hijacker'\n",
      " 'hijacking' 'hostage' 'hostages' 'hurricane' 'injured' 'injuries'\n",
      " 'injury' 'inundated' 'inundation' 'landslide' 'lava' 'lightning'\n",
      " 'loud%20bang' 'mass%20murder' 'mass%20murderer' 'massacre' 'mayhem'\n",
      " 'meltdown' 'military' 'mudslide' 'natural%20disaster'\n",
      " 'nuclear%20disaster' 'nuclear%20reactor' 'obliterate' 'obliterated'\n",
      " 'obliteration' 'oil%20spill' 'outbreak' 'pandemonium' 'panic' 'panicking'\n",
      " 'police' 'quarantine' 'quarantined' 'radiation%20emergency' 'rainstorm'\n",
      " 'razed' 'refugees' 'rescue' 'rescued' 'rescuers' 'riot' 'rioting'\n",
      " 'rubble' 'ruin' 'sandstorm' 'screamed' 'screaming' 'screams' 'seismic'\n",
      " 'sinkhole' 'sinking' 'siren' 'sirens' 'smoke' 'snowstorm' 'storm'\n",
      " 'stretcher' 'structural%20failure' 'suicide%20bomb' 'suicide%20bomber'\n",
      " 'suicide%20bombing' 'sunk' 'survive' 'survived' 'survivors' 'terrorism'\n",
      " 'terrorist' 'threat' 'thunder' 'thunderstorm' 'tornado' 'tragedy'\n",
      " 'trapped' 'trauma' 'traumatised' 'trouble' 'tsunami' 'twister' 'typhoon'\n",
      " 'upheaval' 'violent%20storm' 'volcano' 'war%20zone' 'weapon' 'weapons'\n",
      " 'whirlwind' 'wild%20fires' 'wildfire' 'windstorm' 'wounded' 'wounds'\n",
      " 'wreck' 'wreckage' 'wrecked']\n",
      "221  different unique keywords\n"
     ]
    }
   ],
   "source": [
    "# some examples of keyword\n",
    "print('Some examples of keyword: ',train.keyword.unique() )\n",
    "# how many different locations?\n",
    "print(train.keyword.nunique(),' different unique keywords' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 blanks:  []\n"
     ]
    }
   ],
   "source": [
    "blanks = []  # start with an empty list\n",
    "\n",
    "for i,rv in train[['text']].itertuples():  # iterate over the DataFrame\n",
    "    if type(rv)==str:            # avoid NaN values\n",
    "        if rv.isspace():         # test 'review' for whitespace\n",
    "            blanks.append(i)     # add matching index numbers to the list\n",
    "        \n",
    "print(len(blanks), 'blanks: ', blanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analisis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(df):\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['scores'] = df['text'].apply(lambda review: sid.polarity_scores(review))\n",
    "    \n",
    "    df['positive'] = df['scores'].apply(lambda score_dict: score_dict['pos'])\n",
    "    df['negative'] = df['scores'].apply(lambda score_dict: score_dict['neg'])\n",
    "    df['neutral'] = df['scores'].apply(lambda score_dict: score_dict['neu'])\n",
    "    df['compound']  =df['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "#df = perform_sentiment_analysis(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train['text']\n",
    "y = train['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Na√Øve Bayes:\n",
    "text_clf_nb = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Linear SVC:\n",
    "text_clf_svc = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# LogisticRegression:\n",
    "text_clf_lr = Pipeline([('tfidf', TfidfVectorizer(stop_words=None)),\n",
    "                     ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions_nb = text_clf_nb.predict(X_test)\n",
    "predictions_svc = text_clf_svc.predict(X_test)\n",
    "predictions_lr = text_clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1345  101]\n",
      " [ 395  672]]\n",
      "---------------\n",
      "[[1204  242]\n",
      " [ 276  791]]\n",
      "---------------\n",
      "[[1272  174]\n",
      " [ 314  753]]\n"
     ]
    }
   ],
   "source": [
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions_nb))\n",
    "print ('---------------')\n",
    "print(metrics.confusion_matrix(y_test,predictions_svc))\n",
    "print ('---------------')\n",
    "print(metrics.confusion_matrix(y_test,predictions_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84      1446\n",
      "           1       0.87      0.63      0.73      1067\n",
      "\n",
      "    accuracy                           0.80      2513\n",
      "   macro avg       0.82      0.78      0.79      2513\n",
      "weighted avg       0.81      0.80      0.80      2513\n",
      "\n",
      "---------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      1446\n",
      "           1       0.77      0.74      0.75      1067\n",
      "\n",
      "    accuracy                           0.79      2513\n",
      "   macro avg       0.79      0.79      0.79      2513\n",
      "weighted avg       0.79      0.79      0.79      2513\n",
      "\n",
      "---------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84      1446\n",
      "           1       0.81      0.71      0.76      1067\n",
      "\n",
      "    accuracy                           0.81      2513\n",
      "   macro avg       0.81      0.79      0.80      2513\n",
      "weighted avg       0.81      0.81      0.80      2513\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions_nb))\n",
    "print ('---------------')\n",
    "print(metrics.classification_report(y_test,predictions_svc))\n",
    "print ('---------------')\n",
    "print(metrics.classification_report(y_test,predictions_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission = test['text']\n",
    "\n",
    "predict = text_clf_lr.predict(X_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission=pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  target\n",
       "0    0       1\n",
       "1    2       0\n",
       "2    3       1\n",
       "3    9       0\n",
       "4   11       1\n",
       "5   12       1\n",
       "6   21       0\n",
       "7   22       0\n",
       "8   27       0\n",
       "9   29       0\n",
       "10  30       0\n",
       "11  35       0\n",
       "12  42       0\n",
       "13  43       0\n",
       "14  45       0\n",
       "15  46       0\n",
       "16  47       0\n",
       "17  51       1\n",
       "18  58       0\n",
       "19  60       0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target']=predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submision_1', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
